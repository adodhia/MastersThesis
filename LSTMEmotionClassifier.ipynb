{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/amit/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/amit/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/amit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,Bidirectional,Dense,Conv1D,Flatten,LSTM,GlobalMaxPooling1D,Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>I wanna change my avi but uSanele :(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>MY PUPPY BROKE HER FOOT :(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>where's all the jaebum baby pictures :((</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>But but Mr Ahmad Maslan cooks too :( https://t.co/ArCiD31Zv6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>@eawoman As a Hull supporter I am expecting a misserable few weeks :-(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                               Tweet  \\\n",
       "0     #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)                  \n",
       "1     @Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!   \n",
       "2     @DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!                      \n",
       "3     @97sides CONGRATS :)                                                                                                             \n",
       "4     yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days                       \n",
       "...                                                                                                          ...                       \n",
       "9995  I wanna change my avi but uSanele :(                                                                                             \n",
       "9996  MY PUPPY BROKE HER FOOT :(                                                                                                       \n",
       "9997  where's all the jaebum baby pictures :((                                                                                         \n",
       "9998  But but Mr Ahmad Maslan cooks too :( https://t.co/ArCiD31Zv6                                                                     \n",
       "9999  @eawoman As a Hull supporter I am expecting a misserable few weeks :-(                                                           \n",
       "\n",
       "      Sentiment  \n",
       "0     1          \n",
       "1     1          \n",
       "2     1          \n",
       "3     1          \n",
       "4     1          \n",
       "...  ..          \n",
       "9995 -1          \n",
       "9996 -1          \n",
       "9997 -1          \n",
       "9998 -1          \n",
       "9999 -1          \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data = positive_tweets,columns = ['Tweet'])\n",
    "df['Sentiment'] = 1\n",
    "df2 = pd.DataFrame(data = negative_tweets,columns = ['Tweet'])\n",
    "df2['Sentiment'] = -1\n",
    "df = df.append(df2, ignore_index=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise\n",
    "tknzr = TweetTokenizer()\n",
    "df['Tweet'] = df['Tweet'].apply(tknzr.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lematise Sentence\n",
    "df['Tweet'] = df['Tweet'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tweet_tokens): #, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = df['Tweet'].apply(remove_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     [#followfriday, top, engage, member, community, week, :)]                                             \n",
       "1     [hey, james, odd, :/, please, call, contact, centre, 02392441234, able, assist, :), many, thanks]     \n",
       "2     [listen, last, night, :), bleed, amazing, track, scotland]                                            \n",
       "3     [congrats, :)]                                                                                        \n",
       "4     [yeaaaah, yippppy, accnt, verify, rqst, succeed, get, blue, tick, mark, fb, profile, :), 15, day]     \n",
       "5     [one, irresistible, :), #flipkartfashionfriday]                                                       \n",
       "6     [like, keep, lovely, customer, wait, long, hope, enjoy, happy, friday, lwwf, :)]                      \n",
       "7     [second, thought, ‚Äô, enough, time, dd, :), new, short, enter, system, sheep, must, buy]               \n",
       "8     [jgh, go, bayan, :d, bye]                                                                             \n",
       "9     [act, mischievousness, call, etl, layer, in-house, warehouse, app, katamari, well, ‚Ä¶, name, imply, :p]\n",
       "10    [#followfriday, top, influencers, community, week, :)]                                                \n",
       "11    [love, big, ..., juicy, ..., selfies, :)]                                                             \n",
       "12    [follow, follow, u, back, :)]                                                                         \n",
       "13    [perfect, already, know, what's, wait, :)]                                                            \n",
       "14    [great, new, opportunity, junior, triathletes, age, 12, 13, gatorade, series, get, entry, :)]         \n",
       "15    [laying, greet, card, range, print, today, love, job, :-)]                                            \n",
       "16    [friend's, lunch, ..., yummmm, :), #nostalgia, #tbs, #ku]                                             \n",
       "17    [id, conflict, thanks, help, :d, here's, screenshot, work]                                            \n",
       "18    [hi, liv, :)]                                                                                         \n",
       "19    [hello, need, know, something, u, fm, twitter, ‚Äî, sure, thing, :), dm, x]                             \n",
       "20    [#followfriday, top, new, follower, community, week, :)]                                              \n",
       "21    [i've, hear, four, seasons, pretty, dope, penthouse, obvs, #gobigorgohome, fun, y'all, :)]            \n",
       "22    [yeah, suppose, lol, chat, bit, x, :)]                                                                \n",
       "23    [hello, :), get, youth, job, opportunities, follow]                                                   \n",
       "24    [üíÖ, üèΩ, üíã, :), see, year]                                                                              \n",
       "25    [thank, :-)]                                                                                          \n",
       "26    [hope, rest, night, go, quickly, ..., bed, ..., get, music, fix, time, dream, :)]                     \n",
       "27    [spiritual, ritual, festival, n√©pal, beginning, line-up, :), leave, line-up, see, ...]                \n",
       "28    [hey, sarah, send, us, email, bitsy.com, we'll, help, asap, :)]                                       \n",
       "29    [lols, :d]                                                                                            \n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1) \n",
    "df['Tweet'].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':(', 4586), (':)', 3693), (':-)', 701), (':d', 658), ('...', 622), ('get', 589), (\"i'm\", 526), ('follow', 513), (':-(', 501), ('love', 474), ('thanks', 465), ('like', 447), ('go', 442), ('u', 438), ('good', 418), ('day', 386), ('please', 373), ('want', 342), ('see', 340), ('miss', 318), ('know', 300), ('time', 285), ('back', 284), ('thank', 284), ('one', 280), ('make', 263), ('work', 239), ('..', 237), ('much', 228), (\"can't\", 223), ('today', 220), ('happy', 218), ('really', 212), ('‚ôõ', 210), ('„Äã', 210), ('hi', 207), ('new', 199), ('hope', 196), ('great', 195), ('look', 195), ('think', 192), ('come', 183), ('im', 182), ('need', 181), ('still', 172), ('say', 170), ('sorry', 170), ('feel', 165), ('us', 161), ('<3', 160), ('would', 154), ('oh', 145), (':p', 139), ('week', 138), ('well', 137), ('sleep', 137), (\"i'll\", 132), ('morning', 129), (\"that's\", 128), ('sad', 127), ('wanna', 124), ('people', 124), ('always', 123), ('yes', 122), ('let', 120), ('last', 118), ('thing', 118), ('watch', 118), ('nice', 118), ('wish', 117), ('take', 114), ('night', 113), ('friday', 112), (\"i've\", 112), ('lot', 112), ('x', 111), ('bad', 111), ('lol', 105), ('guy', 105), ('followed', 105), ('keep', 104), ('hey', 103), ('start', 101), ('even', 101), ('could', 101), ('give', 100), ('wait', 98), ('2', 98), ('right', 98), ('1', 97), ('weekend', 97), ('find', 97), ('birthday', 96), ('try', 95), ('tweet', 95), ('help', 94), ('never', 94), ('friend', 94), ('home', 94), ('via', 93)]\n",
      "\n",
      "\n",
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 264), ('thank', 253), ('u', 245), ('day', 242), ('like', 229), ('see', 195), ('happy', 192), (\"i'm\", 183), ('great', 175), ('hi', 173), ('go', 167), ('back', 163), ('know', 162), ('new', 147), ('make', 145), (':p', 139), ('<3', 135), ('one', 131), ('..', 129), ('time', 125), ('hope', 123), ('us', 115), ('look', 114), ('today', 112), ('work', 109), ('friday', 100), ('nice', 99), ('please', 98), ('morning', 98), ('want', 96), (\"i'll\", 91), ('think', 89), ('much', 89), ('lot', 85), ('via', 85), ('would', 84), ('week', 83), ('well', 81), ('let', 81), ('really', 79), ('hey', 77), ('enjoy', 77), ('follower', 77), ('come', 76), ('yes', 76), ('say', 76), ('need', 75), ('birthday', 73), ('x', 72), ('welcome', 72), ('weekend', 72), ('1', 71), ('keep', 68), ('thing', 68), ('night', 67), (\"that's\", 67), ('always', 67), ('arrive', 66), ('best', 64), ('wait', 62), ('lol', 62), ('tweet', 62), ('lovely', 61), ('stats', 60), ('unfollowers', 60), ('friend', 59), ('hello', 58), ('everyone', 58), ('start', 58), ('fun', 57), ('sure', 56), ('2', 55), ('guy', 55), ('im', 54), ('give', 54), ('oh', 53), ('haha', 53), ('check', 52), ('smile', 52), ('life', 51), ('bestfriend', 49), ('take', 49), ('people', 49), ('first', 49), ('next', 48), ('still', 48), ('yeah', 47), ('right', 47), ('soon', 47), ('last', 46), ('sleep', 46), ('way', 46)]\n",
      "\n",
      "\n",
      "[(':(', 4585), (':-(', 501), (\"i'm\", 343), ('...', 332), ('get', 325), ('miss', 291), ('go', 275), ('please', 275), ('want', 246), ('like', 218), ('‚ôõ', 210), ('„Äã', 210), ('u', 193), (\"can't\", 180), ('time', 160), ('follow', 156), ('sorry', 149), ('one', 149), ('see', 145), ('day', 144), ('love', 141), ('much', 139), ('know', 138), ('good', 135), ('really', 133), ('work', 130), ('im', 128), ('feel', 128), ('still', 124), ('sad', 123), ('back', 121), ('make', 118), ('..', 108), ('today', 108), ('come', 107), ('need', 106), ('think', 103), ('followed', 102), ('say', 94), ('wanna', 94), ('oh', 92), ('wish', 91), ('sleep', 91), ('bad', 88), ('look', 81), (\"i've\", 77), ('thanks', 77), ('people', 75), ('watch', 73), ('hope', 73), ('last', 72), ('would', 70), ('could', 69), ('even', 67), ('take', 65), ('leave', 64), ('home', 63), ('omg', 63), ('find', 62), (\"that's\", 61), ('never', 57), ('someone', 57), ('though', 57), ('well', 56), ('always', 56), ('week', 55), ('hate', 53), ('dont', 53), ('try', 53), ('help', 52), ('new', 52), ('already', 52), ('happen', 51), ('gonna', 51), ('right', 51), ('guy', 50), ('long', 50), ('thing', 50), ('mean', 49), ('baby', 47), ('bc', 47), ('cant', 47), ('pls', 47), ('>:(', 47), ('night', 46), ('give', 46), ('yes', 46), ('us', 46), ('soon', 45), ('year', 45), ('cry', 44), ('fuck', 44), ('2', 43), ('start', 43), ('play', 43), ('sick', 43), ('lol', 43), ('stop', 43), ('cute', 42), ('talk', 42)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "#all_words = ' '.join([text for text in cleaned])\n",
    "flattened_tokeninized_final = [i for j in df['Tweet'] for i in j]\n",
    "fdist=FreqDist(flattened_tokeninized_final)\n",
    "print(fdist.most_common(100))\n",
    "print('\\n')\n",
    "\n",
    "pos_all_words = [i for j in df['Tweet'][df['Sentiment'] == 1] for i in j]\n",
    "fdist_pos=FreqDist(pos_all_words)\n",
    "print(fdist_pos.most_common(100))\n",
    "print('\\n')\n",
    "\n",
    "neg_all_words = [i for j in df['Tweet'][df['Sentiment'] == -1] for i in j]\n",
    "fdist_neg=FreqDist(neg_all_words)\n",
    "print(fdist_neg.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tokenized_tweet = df['Tweet']\n",
    "for i in range(len(df['Tweet'])):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the dataframe\n",
    "df = df.sample(frac=1)\n",
    "df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.to_pickle('SentimentTweetsDataset') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4821</th>\n",
       "      <td>see uni .. talent :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4529</th>\n",
       "      <td>think ‚Äô like mate :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6361</th>\n",
       "      <td>young outlive &gt;:(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>fat co probably eat yuna :d :d :d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6730</th>\n",
       "      <td>craving pizza :(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7853</th>\n",
       "      <td>yes that's scary sight see frail people car lottery rid near people always hold breath :(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>u r smart real life u act smart webz :-)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7015</th>\n",
       "      <td>eyyah :( man utd bear ... best</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8928</th>\n",
       "      <td>get sick :(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>aff uname id jonginuh feel free add :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          Tweet  \\\n",
       "4821  see uni .. talent :)                                                                        \n",
       "4529  think ‚Äô like mate :)                                                                        \n",
       "6361  young outlive >:(                                                                           \n",
       "289   fat co probably eat yuna :d :d :d                                                           \n",
       "6730  craving pizza :(                                                                            \n",
       "...                ...                                                                            \n",
       "7853  yes that's scary sight see frail people car lottery rid near people always hold breath :(   \n",
       "1331  u r smart real life u act smart webz :-)                                                    \n",
       "7015  eyyah :( man utd bear ... best                                                              \n",
       "8928  get sick :(                                                                                 \n",
       "2240  aff uname id jonginuh feel free add :)                                                      \n",
       "\n",
       "      Sentiment  \n",
       "4821  1          \n",
       "4529  1          \n",
       "6361 -1          \n",
       "289   1          \n",
       "6730 -1          \n",
       "...  ..          \n",
       "7853 -1          \n",
       "1331  1          \n",
       "7015 -1          \n",
       "8928 -1          \n",
       "2240  1          \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('SentimentTweetsDataset')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_folder = Path(\"data/glove.6B\")\n",
    "file_to_open = data_folder / \"glove.6B.100d.txt\"\n",
    "\n",
    "f = open(file_to_open)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = tokenized_tweet\n",
    "\n",
    "train['Text'] = tokenized_tweet[:7000]\n",
    "test['Text'] = tokenized_tweet[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Sentiment'] = df['Sentiment'][:7000]\n",
    "test['Sentiment'] = df['Sentiment'][7000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size : 9077\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train['Text'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X Shape: (7000, 15)\n",
      "Testing X Shape: (3000, 15)\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 15\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(train.Text),\n",
    "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(test.Text),\n",
    "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Training X Shape:\",x_train.shape)\n",
    "print(\"Testing X Shape:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    8   65   97 1231   80  119   29   67   16   40 3298]\n",
      "like keep lovely customer wait long hope enjoy happy friday lwwf :)\n"
     ]
    }
   ],
   "source": [
    "n = 6\n",
    "print(x_train[n])\n",
    "print(train[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X Shape: (7000, 40)\n",
      "Testing X Shape: (3000, 40)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training X Shape:\",x_train.shape)\n",
    "print(\"Testing X Shape:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train.Sentiment.unique().tolist()\n",
    "train.Sentiment\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train.Sentiment.to_list())\n",
    "y_train = encoder.transform(train.Sentiment.to_list())\n",
    "y_test = encoder.transform(test.Sentiment.to_list())\n",
    "train.Sentiment.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (7000, 1)\n",
      "y_test shape: (3000, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "\n",
    "for line in f:\n",
    "  values = line.split()\n",
    "  word = value = values[0]\n",
    "  coefs = np.asarray(values[1:], dtype='float32')\n",
    "  embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' %len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "  embedding_vector = embeddings_index.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                                          EMBEDDING_DIM,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                          input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                          trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding_sequences = embedding_layer(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "x = Conv1D(64, 5, activation='relu')(x)\n",
    "x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(sequence_input, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "ReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n",
    "                                     min_lr = 0.01,\n",
    "                                     monitor = 'val_loss',\n",
    "                                     verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "7000/7000 [==============================] - 24s 3ms/sample - loss: 0.6950 - acc: 0.4921 - val_loss: 0.6942 - val_acc: 0.5013\n",
      "Epoch 2/10\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.6943 - acc: 0.5004 - val_loss: 0.6940 - val_acc: 0.5003\n",
      "Epoch 3/10\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.6934 - acc: 0.5117 - val_loss: 0.6937 - val_acc: 0.4950\n",
      "Epoch 4/10\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.6921 - acc: 0.5170 - val_loss: 0.6943 - val_acc: 0.4947\n",
      "Epoch 5/10\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 0.6906 - acc: 0.5314 - val_loss: 0.6947 - val_acc: 0.4927\n",
      "Epoch 6/10\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.6884 - acc: 0.5434 - val_loss: 0.6976 - val_acc: 0.4877\n",
      "Epoch 7/10\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.6850 - acc: 0.5559 - val_loss: 0.7006 - val_acc: 0.4803\n",
      "Epoch 8/10\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.6840 - acc: 0.5521 - val_loss: 0.7014 - val_acc: 0.4780\n",
      "Epoch 9/10\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 0.6785 - acc: 0.5634 - val_loss: 0.7037 - val_acc: 0.4773\n",
      "Epoch 10/10\n",
      "7000/7000 [==============================] - 10s 1ms/sample - loss: 0.6748 - acc: 0.5701 - val_loss: 0.7096 - val_acc: 0.4783\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-83d8495e5ce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s, (at, al) = plt.subplots(2,1)\n",
    "at.plot(history.history['accuracy'], c= 'b')\n",
    "at.plot(history.history['val_accuracy'], c='r')\n",
    "at.set_title('model accuracy')\n",
    "at.set_ylabel('accuracy')\n",
    "at.set_xlabel('epoch')\n",
    "at.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n",
    "\n",
    "al.plot(history.history['loss'], c='m')\n",
    "al.plot(history.history['val_loss'], c='c')\n",
    "al.set_title('model loss')\n",
    "al.set_ylabel('loss')\n",
    "al.set_xlabel('epoch')\n",
    "al.legend(['train', 'val'], loc = 'upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 2s 755us/sample\n"
     ]
    }
   ],
   "source": [
    "def decode_sentiment(score):\n",
    "    return \"Positive\" if score>0.5 else \"Negative\"\n",
    "\n",
    "\n",
    "scores = model.predict(x_test, verbose=1, batch_size=1000)\n",
    "y_pred_1d = [decode_sentiment(score) for score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " ...]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=13)\n",
    "    plt.yticks(tick_marks, classes, fontsize=13)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label', fontsize=17)\n",
    "    plt.xlabel('Predicted label', fontsize=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-47f768f75cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_1d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Confusion matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# Check that we don't mix string type with number type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix of label input types (string and number)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(test.Sentiment.to_list(), y_pred_1d)\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(cnf_matrix, classes=test.Sentiment.unique(), title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-12722861092f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_1d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1973\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1975\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1976\u001b[0m         \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# Check that we don't mix string type with number type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix of label input types (string and number)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "print(classification_report(list(test.Sentiment), y_pred_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10\n",
    "max_len = \n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(train['Text'])\n",
    "sequences = tok.texts_to_sequences(train['Text'])\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 50)           5000      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 51,337\n",
      "Trainable params: 51,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 1400 samples\n",
      "Epoch 1/10\n",
      "5600/5600 [==============================] - 43s 8ms/step - loss: 1.0963 - accuracy: 0.5143 - val_loss: 0.6974 - val_accuracy: 0.4814\n",
      "Epoch 2/10\n",
      "5600/5600 [==============================] - 45s 8ms/step - loss: 0.6913 - accuracy: 0.5288 - val_loss: 0.6943 - val_accuracy: 0.5171\n",
      "Epoch 3/10\n",
      "5600/5600 [==============================] - 52s 9ms/step - loss: 0.6903 - accuracy: 0.5289 - val_loss: 0.6970 - val_accuracy: 0.4814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a9c877410>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix,y_train,batch_size=128,epochs=10, validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
